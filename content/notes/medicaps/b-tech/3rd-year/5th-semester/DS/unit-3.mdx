---
title: "Unit 3: Data Science"
description: Exploratory Data Analysis and the Data Science Process - Basic tools (plots, graphs, and summary statistics) of EDA - Philosophy of EDA - The Data Science Process - Case Study
date: 2024-12-31
tags: ["Data Science", "5th Semester", "3rd Year", "Medicaps University"]
published: true
metadata:
  university: "Medicaps University"
  degree: "B Tech"
  semester: "5th Semester"
  subject: "Data Science"
---
---

## **Exploratory Data Analysis (EDA) and the Data Science Process**

### **What is Exploratory Data Analysis (EDA)?**

Exploratory Data Analysis (EDA) is an essential step in the data analysis process where analysts and data scientists examine and understand the structure, patterns, and relationships within the data before applying any modeling techniques. The goal of EDA is to gain insights, identify anomalies, test assumptions, and check for underlying patterns that may guide further analysis.

#### **Key Goals of EDA:**
- **Summarize the main characteristics** of the data.
- **Visualize relationships** between different variables.
- **Identify outliers and anomalies** in the data.
- **Check for missing data** and decide how to handle it.
- **Assess the underlying assumptions** of statistical methods (e.g., normality, homogeneity).

---

### **Steps in Exploratory Data Analysis (EDA):**

#### **1. Data Collection:**
Before performing EDA, you need to collect and load your data. This can be from various sources, including databases, APIs, or file systems (e.g., CSV, Excel).

#### **2. Data Cleaning:**
Cleaning the data is crucial as real-world data is often messy. Steps in data cleaning include:
- **Handling missing values**: You may need to fill them, remove rows, or use statistical imputation.
- **Dealing with duplicates**: Remove any duplicated data points that may distort analysis.
- **Removing errors or inconsistencies**: Fix or remove data entries that do not conform to the expected formats.

#### **3. Data Transformation:**
Data transformation includes actions like:
- **Normalizing or standardizing data**: Scaling data values to a similar range (useful for machine learning models).
- **Encoding categorical variables**: Converting text categories into numerical formats (e.g., one-hot encoding).

#### **4. Summary Statistics:**
EDA begins with basic summary statistics to understand the overall distribution and central tendency of the data. These include:
- **Mean, median, mode** (measures of central tendency).
- **Variance, standard deviation** (measures of spread).
- **Quantiles, percentiles** (e.g., 25th, 50th, 75th percentile).
  
#### **5. Visualizing the Data:**
Visualization is a powerful tool in EDA. Some common types of visualizations include:
- **Histograms**: Show the frequency distribution of numerical data.
- **Boxplots**: Display the distribution and identify outliers.
- **Scatter Plots**: Display relationships between two continuous variables.
- **Correlation Heatmaps**: Show the relationship between multiple variables at once.
  
#### **Example:**

- If you have data on students' scores in different subjects, a histogram could help you understand the distribution of scores (whether most students scored high or low).
- A scatter plot could help you analyze if there's a correlation between the hours of study and the exam scores.

---

### **Common EDA Tools and Libraries:**
- **Python Libraries:** Pandas, Matplotlib, Seaborn, Plotly, NumPy
- **R Libraries:** ggplot2, dplyr, tidyr, plotly
- **Excel**: Pivot tables, basic charts

---

### **The Data Science Process**

The data science process is a structured approach used to extract insights and value from data. It consists of several stages, which are iterative and may require revisiting previous steps as new information or challenges arise.

#### **1. Problem Definition:**
The first step is to define the **problem** clearly. You need to understand the goals of the analysis:
- What question are you trying to answer?
- What is the objective of the analysis (prediction, classification, optimization, etc.)?

#### **2. Data Collection:**
Once the problem is defined, the next step is to **gather relevant data**. Data can come from multiple sources:
- Internal data (company databases, logs).
- External data (web scraping, APIs, public datasets).

#### **3. Data Cleaning:**
Data is rarely in a clean, usable form. Therefore, **data cleaning** is a crucial step in the process to ensure data quality and integrity. This includes:
- Handling missing or incorrect data.
- Removing duplicates and fixing inconsistencies.

#### **4. Exploratory Data Analysis (EDA):**
As discussed earlier, EDA is the stage where data scientists explore the data visually and statistically. The main goal is to uncover hidden patterns, relationships, and potential insights that can help inform further analysis.

#### **5. Feature Engineering:**
This step involves creating new variables (features) from the existing data to improve the performance of machine learning models. Examples include:
- **Creating interaction terms** between features.
- **Transforming variables** (e.g., logarithmic or polynomial transformations).

#### **6. Model Building:**
In this stage, data scientists select appropriate machine learning or statistical models to address the problem. This involves:
- Choosing algorithms (e.g., regression, classification, clustering).
- Training the models on historical data (training data).
- Evaluating model performance using metrics such as accuracy, precision, recall, and F1 score.

#### **7. Model Evaluation:**
Once the model is trained, it's time to evaluate its performance using unseen test data. You assess how well the model generalizes to new data and whether it meets the desired accuracy or other performance measures.

#### **8. Model Deployment and Monitoring:**
After evaluation, the best-performing model is deployed into a real-world environment. The model is continuously monitored to ensure that it remains effective over time. This may involve:
- **Model updating**: Retraining the model periodically with new data.
- **Performance monitoring**: Ensuring the model maintains accuracy and reliability.

#### **9. Communicating Results:**
The final step is to **present findings** in a clear and actionable way to stakeholders. This may include creating reports, dashboards, or data visualizations that explain the results and help guide decision-making.

---

### **EDA and the Data Science Process: Key Points**

| **Step**                  | **Purpose**                                   | **Example Tools**                              |
|---------------------------|-----------------------------------------------|------------------------------------------------|
| **Problem Definition**     | Define the goal and objectives of the analysis | Business stakeholder meetings                 |
| **Data Collection**        | Gather relevant data                          | Web scraping, APIs, Databases                 |
| **Data Cleaning**          | Handle missing, inconsistent, and duplicate data | Pandas, OpenRefine                            |
| **Exploratory Data Analysis (EDA)** | Summarize and visualize data | Matplotlib, Seaborn, Plotly, ggplot2          |
| **Feature Engineering**    | Create new features for modeling              | Feature transformations, polynomial features  |
| **Model Building**         | Choose and train machine learning models      | Scikit-learn, TensorFlow, XGBoost             |
| **Model Evaluation**       | Assess model performance                      | Confusion matrix, ROC curves, Cross-validation |
| **Model Deployment**       | Deploy the model for real-time use            | Flask, Docker, Cloud services (AWS, GCP)      |
| **Communicating Results**  | Present results to stakeholders               | Reports, Dashboards, PowerPoint presentations |

---

💡 **TIP:** EDA should be an ongoing process throughout the data science workflow. Insights gained during EDA can lead to changes in the model or features, which can improve the final outcomes.

---

## **Basic Tools (Plots, Graphs, and Summary Statistics) of Exploratory Data Analysis (EDA)**

### **1. Summary Statistics**

Summary statistics provide a quick and concise overview of the dataset, helping us understand the data’s central tendency, spread, and distribution. These are the fundamental building blocks of EDA.

#### **Key Summary Statistics:**

- **Measures of Central Tendency:**
  - **Mean**: The average value of the dataset.
  $$ \text{Mean} = \frac{\sum_{i=1}^n X_i}{n} $$

  - **Median**: The middle value of the dataset when sorted in ascending or descending order. If the dataset has an even number of observations, the median is the average of the two middle numbers.

  - **Mode**: The value that occurs most frequently in the dataset.

- **Measures of Dispersion:**
  - **Range**: The difference between the maximum and minimum values in the dataset.
  $$ \text{Range} = \text{Max} - \text{Min} $$

  - **Variance**: The average of the squared differences from the mean, providing a measure of how spread out the data is.
  $$ \text{Variance} = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 $$

  - **Standard Deviation (SD)**: The square root of the variance, giving the spread of data in the same units as the data itself.
  $$ \text{SD} = \sqrt{\text{Variance}} $$

- **Percentiles and Quantiles:**
  - **25th, 50th, and 75th percentiles** (also known as quartiles): Help describe the distribution and give insights into the spread of the data.
  - **Interquartile Range (IQR)**: The range between the 25th and 75th percentiles.
  $$ \text{IQR} = Q3 - Q1 $$

#### **Example:**
For a dataset of student scores: [50, 60, 70, 80, 90]

- **Mean**: $$ \frac{50 + 60 + 70 + 80 + 90}{5} = 70 $$
- **Median**: 70 (middle value when sorted).
- **Range**: 90 - 50 = 40.

---

### **2. Plots and Graphs in EDA**

Graphs and plots are powerful tools for visualizing data distributions, relationships, and patterns. They help in understanding the underlying structure of the data, identifying outliers, and revealing trends.

#### **Types of Plots in EDA:**

#### **1. Histograms:**

A histogram is used to display the distribution of a single numerical variable. It divides the data into bins or intervals and shows how many data points fall into each bin.

- **Purpose**: To see the frequency distribution and shape of the data.
- **Interpretation**: Helps identify skewness, peaks, and the spread of the data.

**Example:**
A histogram of exam scores for students would show how many students scored within certain score ranges (e.g., 50-60, 60-70).

#### **2. Boxplots (Box-and-Whisker Plots):**

A boxplot visually represents the distribution of a dataset through its quartiles and shows outliers. It includes:
- **Box**: Contains the interquartile range (IQR) between the 25th and 75th percentiles.
- **Whiskers**: Extend from the box to the maximum and minimum values, excluding outliers.
- **Outliers**: Points outside the whiskers are often considered outliers.

- **Purpose**: To show the spread, central tendency, and presence of outliers in the data.

**Example:**
A boxplot of salaries in an organization could highlight the median salary, interquartile range, and any extreme values (outliers).

#### **3. Scatter Plots:**

A scatter plot displays the relationship between two continuous variables. Each point represents a pair of values.

- **Purpose**: To check for correlations or relationships between two variables.
- **Interpretation**: If the points follow a straight line or a curve, it suggests a potential correlation.

**Example:**
A scatter plot showing the relationship between hours studied and exam scores.

#### **4. Correlation Heatmap:**

A heatmap displays the correlation matrix of several variables. It uses colours to represent the strength of the correlation:
- **Red**: Strong positive correlation.
- **Blue**: Strong negative correlation.
- **White/Light**: No correlation.

- **Purpose**: To visually assess the relationships between multiple variables in a dataset.

**Example:**
A correlation heatmap of sales data for different products might show which products tend to be bought together.

#### **5. Bar Charts:**

Bar charts are used to represent categorical data. Each bar represents the frequency or percentage of a category in the dataset.

- **Purpose**: To compare the counts or percentages of different categories.
- **Interpretation**: Helps identify the most and least common categories.

**Example:**
A bar chart showing the number of students in each department of a university.

#### **6. Pair Plots:**

A pair plot is a grid of scatter plots that shows pairwise relationships between variables. It's particularly useful when exploring relationships between multiple variables.

- **Purpose**: To visualize the relationships between several continuous variables at once.

**Example:**
A pair plot of students' scores in math, science, and English to explore their relationships.

---

### **3. Correlation and Covariance:**

- **Correlation**: Measures the strength and direction of the linear relationship between two variables. It is measured by the **Pearson correlation coefficient**, ranging from -1 to 1.
  - **+1** indicates a perfect positive correlation.
  - **-1** indicates a perfect negative correlation.
  - **0** indicates no linear relationship.
  
  $$ \text{Pearson's correlation} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} $$

- **Covariance**: Measures the directional relationship between two variables. It tells us whether two variables tend to increase together (positive covariance) or if one increases while the other decreases (negative covariance).
  
  $$ \text{Cov}(X, Y) = \frac{\sum (X_i - \mu_X)(Y_i - \mu_Y)}{n} $$

---

### **Example: Visualizing EDA Tools in Action**

Consider a dataset of housing prices with features like area, number of bedrooms, and age of the house. You would:

1. **Use Summary Statistics** to check the central tendency and variability of house prices.
2. **Create a Histogram** to understand the distribution of house prices.
3. **Generate a Boxplot** to identify potential outliers in prices.
4. **Plot a Scatter Plot** to see the relationship between price and area of the house.
5. **Create a Correlation Heatmap** to check if the number of bedrooms and house age correlate with price.

---

💡 **TIP:** Always start EDA with summary statistics and visualizations before diving into modeling. This helps you understand the data and make informed decisions about data transformation or feature engineering.

---

## **Philosophy of Exploratory Data Analysis (EDA)**

Exploratory Data Analysis (EDA) is not merely a set of techniques or methods used in data analysis; it is a **philosophy**—a mindset and approach to understanding data. The core idea of EDA is to **explore**, **visualize**, and **hypothesize** about the data before making any assumptions or jumping into complex analyses or model building. The primary goal is to **gain deep insights** into the data through visualizations, summary statistics, and statistical methods.

### **Key Principles of the Philosophy of EDA:**

#### **1. Understand the Data Before Modeling**
One of the central philosophies of EDA is the **need to understand the data thoroughly** before applying any models or advanced statistical methods. This understanding is built through a careful exploration of the data’s structure, relationships, and underlying patterns. 

- **Why it matters:** Models may be misleading or even inappropriate if you do not understand the nature of the data. For example, applying a linear regression model to a dataset with non-linear relationships may lead to inaccurate predictions.

- **Philosophical standpoint:** *“You cannot model data you do not understand."*

#### **2. Visualize Data to Discover Patterns and Relationships**
Visualization is a key tool in the EDA philosophy. The act of visualizing data, whether through histograms, scatter plots, boxplots, or heatmaps, allows analysts to **see patterns** that are not apparent through raw numbers alone. This allows for:
- Detecting outliers and anomalies.
- Identifying trends, correlations, or associations.
- Assessing the distribution and spread of data.

- **Why it matters:** Visualizations often reveal **relationships between variables**, the **shape of the data**, and unexpected patterns, helping guide further analysis.

- **Philosophical standpoint:** *“A picture is worth a thousand numbers.”*

#### **3. Encourage Hypothesis-Driven Exploration**
EDA encourages an **iterative, hypothesis-driven exploration** of data. Rather than jumping straight into statistical analysis or machine learning, analysts start by asking questions and forming hypotheses about the data. 

- **For example:** If you suspect a relationship between two variables, you might begin by plotting them and then examining the correlation to test that hypothesis.
  
- **Why it matters:** This hypothesis-driven approach fosters a **deeper, more meaningful understanding** of the data. It ensures that insights are driven by **informed intuition** and not just blind analysis.

- **Philosophical standpoint:** *“Ask questions first, let the data answer.”*

#### **4. Be Open to Surprises and Non-Linear Thinking**
EDA encourages analysts to **remain open-minded and flexible** during the exploration process. Often, data contains unexpected or surprising insights that challenge preconceived notions or assumptions. By adopting a mindset of **curiosity** and being willing to entertain different hypotheses, analysts can uncover important patterns that may have been overlooked.

- **Why it matters:** Data does not always follow expected or linear patterns. Surprises are a natural part of EDA and can provide valuable insights, particularly when analyzing complex or real-world datasets.
  
- **Philosophical standpoint:** *“Data may surprise you; be open to discovering the unknown.”*

#### **5. Emphasize Simplicity and Clarity**
The philosophy of EDA also advocates for **simplicity and clarity**. While EDA can use complex statistical methods, the goal is to uncover clear and actionable insights in a simple and understandable manner. 

- **Why it matters:** Simple, easy-to-understand results can be more impactful and easier to communicate to stakeholders or decision-makers who might not have technical expertise.

- **Philosophical standpoint:** *“Simplicity is the ultimate sophistication.”* — Leonardo da Vinci

#### **6. Iterative Process**
EDA is **not a one-time task** but an **iterative process**. Analysts frequently revisit and refine their visualizations, summary statistics, and hypotheses as new insights emerge. With each iteration, the understanding of the data deepens, and more meaningful insights are generated.

- **Why it matters:** Data analysis is an evolving process, and new questions often arise as you explore the data. This iterative nature of EDA allows analysts to continuously refine their understanding and adapt to new information.

- **Philosophical standpoint:** *“Exploration is an ongoing journey, not a destination.”*

---

### **Philosophy in Practice: How EDA Guides the Data Science Process**

The philosophy of EDA is deeply intertwined with the overall **data science process**. Here’s how it fits into the broader context:

- **Data Understanding (Initial Exploration):** EDA serves as the first and foundational step in the data science process. Before jumping into modeling or predictive analytics, it’s crucial to explore the data thoroughly to ensure that the right techniques will be used later on.

- **Insight Generation:** The insights gained from EDA can guide subsequent steps in data preprocessing, feature engineering, and model selection. For instance, recognizing the presence of outliers might suggest the need for data transformation or robust models.

- **Iterative Model Refinement:** EDA is a cyclical process. As the model is refined and tested, the data is often revisited, and the findings from EDA are integrated into the modeling process.

- **Communication with Stakeholders:** The clarity of insights derived from EDA helps in communicating the data story effectively to non-technical stakeholders. Visualizations, summary statistics, and hypotheses are powerful tools for data storytelling.

---

### **Key Takeaways: The Philosophy of EDA**
- **Empathy with Data**: EDA teaches analysts to treat data as a **conversation partner**, where every chart, graph, and statistic reveals something new.
  
- **The Importance of Curiosity**: The heart of EDA is an unrelenting curiosity about the data. Data exploration is about **asking questions, testing ideas, and learning** from the data itself, not just applying pre-made models.
  
- **Simplicity and Transparency**: The most profound insights often come from simple observations. EDA encourages analysts to focus on transparency and clarity, making the results accessible to all stakeholders.
  
- **Flexibility**: The data science process is flexible, and EDA reflects this by emphasizing the importance of revisiting and refining ideas as new patterns emerge.

---

💡 **TIP:** Always approach EDA with an open mind and a questioning attitude. The journey of data exploration is as much about discovery as it is about analysis.

---

## **The Data Science Process - Case Study**

The data science process is a systematic and structured approach to solving problems using data. It involves a sequence of steps, from understanding the problem to deploying the model. Below is a case study that illustrates the data science process in action.

### **Case Study: Predicting House Prices**

#### **Scenario:**
You are tasked with developing a predictive model to estimate the prices of houses based on various features such as the area of the house, the number of bedrooms, age of the house, and location.

---

### **1. Problem Definition**

The first step in any data science project is to clearly define the problem. Understanding the problem is critical to guide subsequent decisions.

- **Objective:** Predict the price of a house.
- **Output:** A model that can predict house prices based on the input features.
- **Key Stakeholders:** Real estate agents, home buyers, sellers, and property analysts.

#### **Key Questions:**
- What factors contribute most to the price of a house?
- What historical data do we have on house prices?

---

### **2. Data Collection**

Once the problem is defined, the next step is to collect relevant data. In this case, the data needs to include house features (area, number of bedrooms, age, location, etc.) and their corresponding prices.

- **Data Source:** Real estate websites, government databases, or proprietary datasets.
- **Data Type:** Structured data (tables with rows and columns).

#### **Example:**
A sample dataset might look like this:

| House ID | Area (sq ft) | Bedrooms | Age (years) | Location      | Price (USD) |
|----------|--------------|----------|-------------|---------------|-------------|
| 1        | 1500         | 3        | 10          | Downtown      | 350,000     |
| 2        | 2000         | 4        | 5           | Suburban      | 450,000     |
| 3        | 1000         | 2        | 15          | Countryside   | 200,000     |
| 4        | 1800         | 3        | 8           | Downtown      | 400,000     |

---

### **3. Data Cleaning and Preprocessing**

Data often comes with missing values, outliers, or irrelevant features. Preprocessing helps clean the data and prepare it for analysis.

#### **Steps:**
- **Handle Missing Data:** Impute missing values or remove rows with missing values.
- **Remove Outliers:** Identify and remove any data points that fall far outside the expected range.
- **Feature Engineering:** Create new features based on existing ones (e.g., converting categorical features like location into numerical values using one-hot encoding).
- **Normalize/Scale Data:** Some algorithms, like linear regression, benefit from normalized data.
  
#### **Example:**
- **Handling Missing Values:** If some house entries have missing price information, either impute the price based on similar properties or remove these entries.
- **Feature Transformation:** Convert the categorical feature 'Location' into numerical values (e.g., Downtown = 1, Suburban = 2, Countryside = 3).

---

### **4. Exploratory Data Analysis (EDA)**

EDA is a crucial part of the data science process, where we visually explore the data to understand its structure, patterns, and relationships.

#### **Steps in EDA:**
- **Univariate Analysis:** Examine individual features like area, number of bedrooms, etc., using histograms or boxplots.
- **Bivariate Analysis:** Explore relationships between pairs of features, like area vs. price, using scatter plots or correlation matrices.
- **Detect Outliers:** Identify any data points that deviate significantly from others.

#### **Example:**
- **Correlation Analysis:** A scatter plot of area vs. price may show a positive correlation (larger houses tend to have higher prices).
- **Boxplots:** A boxplot of prices might reveal if there are any outliers in the dataset.

---

### **5. Model Building**

This is the stage where the data is used to build machine learning models. Several models can be tested, and the best one will be selected based on performance metrics.

#### **Steps:**
- **Split the Data:** Divide the dataset into a training set (to train the model) and a test set (to evaluate the model's performance).
- **Select a Model:** Choose a model based on the problem (e.g., linear regression for predicting prices).
- **Train the Model:** Train the model on the training data.
- **Evaluate the Model:** Test the model on the test set and evaluate its performance using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or R-squared.

#### **Example:**
- **Model:** Linear Regression
  - **Training:** Train a linear regression model using the area, number of bedrooms, and other features to predict the house price.
  - **Evaluation:** After training, the model might predict the price for house ID 1 as $340,000, and the actual price is $350,000. The error would be calculated and analyzed.

---

### **6. Model Evaluation**

After building the model, it is essential to evaluate its performance to see how well it predicts the target variable (house price). Evaluation metrics depend on the nature of the problem.

#### **Key Metrics for Regression:**
- **Mean Absolute Error (MAE):** The average of the absolute differences between predicted and actual values.
- **Mean Squared Error (MSE):** The average of the squared differences between predicted and actual values.
- **R-squared:** Measures how well the model explains the variance in the target variable.

#### **Example:**
- **MAE:** If the model's predictions are consistently off by $10,000, the MAE would be $10,000.
- **R-squared:** A high R-squared value (close to 1) indicates a good fit of the model.

---

### **7. Model Tuning and Optimization**

Based on the initial performance of the model, further tuning may be required to improve its accuracy. This includes adjusting model parameters or trying different algorithms.

#### **Steps:**
- **Hyperparameter Tuning:** Adjust parameters such as the learning rate, number of trees (in decision trees or random forests), or the degree of polynomial features.
- **Cross-validation:** Use cross-validation to ensure the model generalizes well to unseen data.

#### **Example:**
- If you used **Random Forests**, you might tune the number of trees or the maximum depth of the trees to avoid overfitting or underfitting.

---

### **8. Model Deployment**

Once the model has been trained and tuned, it’s time to deploy it in a real-world environment, where it can start making predictions.

#### **Steps:**
- **Deploy the Model:** Deploy the trained model into a production environment (e.g., a web application that predicts house prices based on input features).
- **Monitor the Model:** Keep track of the model's performance over time to ensure it continues to provide accurate predictions.
- **Update the Model:** As new data comes in, retrain the model periodically to ensure its performance remains optimal.

#### **Example:**
- The house price prediction model could be integrated into a real estate website, where users can enter details about a house, and the system will predict its price.

---

### **9. Communication and Reporting**

Effective communication is critical throughout the data science process. Once the model is deployed, communicating the results and insights clearly to stakeholders is essential.

#### **Steps:**
- **Create Reports:** Summarize the key findings, model performance, and any limitations.
- **Present Visualizations:** Use visual aids like charts, graphs, and heatmaps to explain the model's behavior and results.
- **Provide Actionable Insights:** Offer suggestions for business decisions based on the model’s predictions.

#### **Example:**
- A report might state: "Based on the analysis of the dataset, the most significant factors influencing house prices are the area, number of bedrooms, and location."

---

### **Key Takeaways from the Case Study:**

- **Iterative Process:** Data science is not linear. The process involves revisiting steps like data cleaning, model building, and evaluation.
- **Exploration is Critical:** The more you explore your data, the better your model will be.
- **Model Evaluation is Crucial:** Evaluating your model helps you understand its limitations and make improvements.

---

💡 **TIP:** While building predictive models, always validate your results on unseen data to ensure your model performs well in real-world scenarios.

---

## Quiz Time

<Quiz 
  questions={[
    { 
      question: "What is the primary goal of EDA?", 
      options: ["Predict data trends", "Understand data structure", "Deploy models", "Train machine learning models"], 
      correctIndex: 1 
    },
    { 
      question: "Which visualization identifies outliers?", 
      options: ["Scatter Plot", "Histogram", "Boxplot", "Bar Chart"], 
      correctIndex: 2 
    },
    { 
      question: "What does data cleaning include?", 
      options: ["Data normalization", "Building models", "Handling missing values", "Creating visualizations"], 
      correctIndex: 2 
    },
    { 
      question: "What is feature engineering?", 
      options: ["Visualizing data", "Deploying models", "Creating new variables", "Collecting data"], 
      correctIndex: 2 
    },
    { 
      question: "Which step assesses model performance?", 
      options: ["Feature Engineering", "Model Building", "Model Evaluation", "Data Collection"], 
      correctIndex: 2 
    },
    { 
      question: "Which graph shows variable relationships?", 
      options: ["Scatter Plot", "Bar Chart", "Boxplot", "Histogram"], 
      correctIndex: 0 
    },
    { 
      question: "What does a correlation heatmap reveal?", 
      options: ["Data outliers", "Variable relationships", "Categorical values", "Model accuracy"], 
      correctIndex: 1 
    },
    { 
      question: "Which metric measures model errors?", 
      options: ["Mean Absolute Error", "Mean Median", "Standard Deviation", "R-squared"], 
      correctIndex: 0 
    },
    { 
      question: "What’s the first step in the data science process?", 
      options: ["Data Cleaning", "Model Deployment", "Problem Definition", "Feature Engineering"], 
      correctIndex: 2 
    },
    { 
      question: "Which Python library is common in EDA?", 
      options: ["TensorFlow", "Pandas", "Keras", "OpenCV"], 
      correctIndex: 1 
    }
  ]}
/>

