---
title: "Unit 2: Data Science"
description: Introduction to Data Science
date: 2024-12-31
tags: ["Data Science", "5th Semester", "3rd Year", "Medicaps University"]
published: true
---

### Syllabus

```Syllabus
Sample spaces, events, Conditional probability, and independence
Random variables: Discrete and Continuous random variables
densities and distributions
Normal distribution and its properties
Introduction to Markov chains, random walks
Descriptive, Predictive, and prescriptive statistics
Statistical Inference
Populations and samples
Statistical modelling
```
---

### Sample Spaces, Events, Conditional Probability, and Independence

#### **Sample Space**

The **sample space** is the set of all possible outcomes of an experiment or random trial. In probability theory, the sample space helps define all potential outcomes that can occur.

- **Example:** If a die is rolled, the sample space \( S \) is {1, 2, 3, 4, 5, 6} because those are the possible outcomes.

#### **Events**

An **event** is any subset of the sample space. It represents a set of outcomes that we are interested in. Events can be simple or complex.

- **Simple Event:** An event that consists of a single outcome from the sample space.  
  - Example: Rolling a 3 on a die.
  
- **Compound Event:** An event that consists of multiple outcomes from the sample space.  
  - Example: Rolling an even number on a die (i.e., {2, 4, 6}).

- **Notation:** Events are often denoted by uppercase letters, such as \( A \), \( B \), or \( C \).

#### **Conditional Probability**

**Conditional probability** is the probability of an event occurring, given that another event has already occurred. It is represented as \( P(A|B) \), which reads as "the probability of event \( A \) given that event \( B \) has occurred."

<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <mi>A</mi>
    <mo>|</mo>
    <mi>B</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mfrac linethickness="0">
      <mrow>
        <mi>P</mi>
        <mo>(</mo>
        <mi>A</mi>
        <mo>&#x2229;</mo>
        <mi>B</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mi>P</mi>
        <mo>(</mo>
        <mi>B</mi>
        <mo>)</mo>
      </mrow>
    </mfrac>
  </mrow>
</math>

- **Example:**  
  If you know that it’s raining, the probability of carrying an umbrella (event \( A \)) is higher. The conditional probability would reflect the likelihood of event \( A \) occurring given that event \( B \) (rain) has occurred.

#### **Independence**

Two events are considered **independent** if the occurrence of one does not affect the occurrence of the other. In other words, the probability of event \( A \) occurring is the same regardless of whether event \( B \) occurs or not.

- **Formula for Independence:**  
  \[
  P(A \cap B) = P(A) \cdot P(B)
  \]
  If the above condition is true, events \( A \) and \( B \) are independent.

- **Example:**  
  If you flip a coin and roll a die, the outcome of the coin flip (heads or tails) does not affect the outcome of the die roll (1 to 6). Therefore, the two events are independent.

---

### Random Variables: Discrete and Continuous Random Variables

#### **Random Variables**

A **random variable** is a variable whose value is subject to variations due to chance. It is a function that assigns a numerical value to each outcome in the sample space of a random experiment.

Random variables are typically classified into two main types:
1. **Discrete Random Variables**
2. **Continuous Random Variables**

---

#### **Discrete Random Variables**

A **discrete random variable** is one that can take a finite or countably infinite number of distinct values. These values are usually countable and often represent counts of items or events.

- **Example:** The number of heads obtained when flipping a coin 3 times. The possible values are 0, 1, 2, or 3 heads.
  
- **Properties:**
  - Discrete random variables are typically integers.
  - Their values are distinct and separable.
  - Probability distribution for discrete random variables is expressed as a **probability mass function (PMF)**.

- **Probability Mass Function (PMF):**
  A PMF gives the probability that a discrete random variable is equal to a specific value.
  
  \[
  P(X = x) = p_x
  \]
  Where:
  - \( X \) is the random variable,
  - \( x \) is a possible value of \( X \),
  - \( p_x \) is the probability of \( X \) taking the value \( x \).

- **Example:**  
  For a fair die, the discrete random variable \( X \) represents the outcome of the die roll. The PMF is:
  \[
  P(X = x) = \frac{1}{6} \quad \text{for} \quad x \in \{1, 2, 3, 4, 5, 6\}
  \]

---

#### **Continuous Random Variables**

A **continuous random variable** is one that can take an infinite number of possible values within a given range. These values are uncountably infinite and can be any real number in an interval.

- **Example:** The height of a person. It can take any value within a range, such as between 150 cm and 200 cm, with infinite possible values in between.

- **Properties:**
  - Continuous random variables are usually real numbers.
  - Their values are not distinct and can be measured with any level of precision.
  - Probability distribution for continuous random variables is described by a **probability density function (PDF)**.

- **Probability Density Function (PDF):**
  The PDF gives the probability of the random variable taking a value in a specific interval. The area under the curve between two points represents the probability of the variable falling within that interval.
  
  \[
  P(a \leq X \leq b) = \int_a^b f(x) dx
  \]
  Where:
  - \( f(x) \) is the probability density function,
  - \( P(a \leq X \leq b) \) is the probability that \( X \) lies between \( a \) and \( b \).

- **Example:**  
  The height of a person \( X \) might follow a normal distribution with a mean of 170 cm and a standard deviation of 10 cm. The PDF would describe the distribution of heights for a population.

---

### Densities and Distributions

#### **Probability Density Function (PDF)**

For a continuous random variable, the **Probability Density Function (PDF)** is a function that describes the likelihood of the random variable taking a particular value in a given range. Unlike discrete variables, where you can assign probabilities to specific values, for continuous variables, the PDF describes the probability over intervals.

- **Formula:**  
  \[
  P(a \leq X \leq b) = \int_a^b f(x) \, dx
  \]
  Where:
  - \( f(x) \) is the probability density function,
  - \( P(a \leq X \leq b) \) is the probability that \( X \) lies between \( a \) and \( b \).

- **Properties of PDF:**
  1. The PDF is always non-negative: \( f(x) \geq 0 \) for all \( x \).
  2. The total area under the PDF curve is equal to 1:  
     \[
     \int_{-\infty}^{\infty} f(x) \, dx = 1
     \]
  3. The probability that the random variable takes a value in an interval is the area under the PDF curve for that interval.

#### **Cumulative Distribution Function (CDF)**

The **Cumulative Distribution Function (CDF)** represents the probability that a random variable \( X \) is less than or equal to a particular value \( x \). It is the integral of the PDF from \( -\infty \) to \( x \).

- **Formula:**  
  \[
  F(x) = P(X \leq x) = \int_{-\infty}^x f(t) \, dt
  \]
  Where:
  - \( F(x) \) is the CDF of the random variable \( X \),
  - \( f(t) \) is the PDF of \( X \).

- **Properties of CDF:**
  1. The CDF is a non-decreasing function.
  2. \( F(x) \) approaches 0 as \( x \to -\infty \) and approaches 1 as \( x \to \infty \).
  3. The CDF is always between 0 and 1: \( 0 \leq F(x) \leq 1 \).

#### **Common Probability Distributions**

Several common probability distributions describe the behavior of random variables in various contexts. Some of the most widely used distributions include:

1. **Normal Distribution (Gaussian Distribution):**
   - The normal distribution is one of the most important continuous distributions. It is symmetric and bell-shaped.
   - **Characteristics:**  
     - Mean \( \mu \) and standard deviation \( \sigma \).
     - The PDF of a normal distribution is given by:
       \[
       f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
       \]
     - The area under the curve represents probabilities.
   - **Applications:**  
     - Measurement errors, height, weight, and many natural phenomena.

2. **Exponential Distribution:**
   - The exponential distribution is often used to model the time between events in a Poisson process (e.g., time between arrivals in a queue).
   - **PDF:**  
     \[
     f(x; \lambda) = \lambda e^{-\lambda x}, \quad x \geq 0
     \]
     Where \( \lambda \) is the rate parameter.
   - **Applications:**  
     - Modeling the time between events (e.g., waiting times).

3. **Uniform Distribution:**
   - The uniform distribution describes a situation where all outcomes are equally likely within a certain interval.
   - **PDF:**  
     \[
     f(x) = \frac{1}{b - a} \quad \text{for} \quad a \leq x \leq b
     \]
   - **Applications:**  
     - Simulations, random number generation.

4. **Poisson Distribution:**
   - The Poisson distribution models the number of events that occur in a fixed interval of time or space.
   - **PDF:**  
     \[
     P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} \quad k = 0, 1, 2, \dots
     \]
     Where \( \lambda \) is the average number of events in the given interval.
   - **Applications:**  
     - Modeling the number of occurrences of an event, such as accidents, calls in a call center, etc.

#### **Properties of Distributions**

- **Mean (Expected Value):**
  The mean of a random variable \( X \), denoted \( E(X) \), represents the central tendency or average value. For a continuous random variable:
  \[
  E(X) = \int_{-\infty}^{\infty} x f(x) \, dx
  \]

- **Variance:**
  The variance of a random variable \( X \), denoted \( Var(X) \), measures how much the values of \( X \) deviate from the mean. For a continuous random variable:
  \[
  Var(X) = E[(X - \mu)^2] = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) \, dx
  \]
  Where \( \mu \) is the mean.

---

### Normal Distribution and Its Properties

#### **Normal Distribution**

The **Normal Distribution** (also known as the Gaussian distribution) is one of the most important and widely used continuous probability distributions. It is symmetric and bell-shaped, and it describes many natural phenomena, such as heights, test scores, and measurement errors.

- **Mathematical Expression:**

  The probability density function (PDF) of a normal distribution is given by:

  \[
  f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
  \]
  Where:
  - \( \mu \) is the **mean** (average) of the distribution,
  - \( \sigma \) is the **standard deviation** (a measure of the spread of the distribution),
  - \( e \) is the base of the natural logarithm.

  This equation describes the shape of the normal distribution, where the mean \( \mu \) represents the centre, and the spread is determined by the standard deviation \( \sigma \).

---

#### **Properties of Normal Distribution**

1. **Symmetry:**
   The normal distribution is perfectly symmetric around the mean. This means that the left side of the curve is a mirror image of the right side. As a result, the mean, median, and mode all coincide at the same point, which is at the centre of the distribution.

   - **Mean = Median = Mode**

2. **Bell-shaped Curve:**
   The curve of the normal distribution is bell-shaped and approaches zero as \( x \) moves further away from the mean. This means that extreme values are less likely to occur than values near the mean.

3. **68-95-99.7 Rule (Empirical Rule):**
   For a normal distribution:
   - About **68%** of the data falls within **1 standard deviation** (\( \mu \pm \sigma \)) of the mean.
   - About **95%** of the data falls within **2 standard deviations** (\( \mu \pm 2\sigma \)) of the mean.
   - About **99.7%** of the data falls within **3 standard deviations** (\( \mu \pm 3\sigma \)) of the mean.

   This rule helps in understanding the spread and distribution of data.

4. **Asymptotic to the x-axis:**
   The tails of the normal distribution curve extend infinitely in both directions, getting closer and closer to the x-axis but never touching it. This implies that the probability of extreme values (far from the mean) never becomes zero.

5. **Mean and Standard Deviation Control the Shape:**
   - The **mean** (\( \mu \)) determines the centre of the distribution.
   - The **standard deviation** (\( \sigma \)) determines the width of the curve:
     - A **small standard deviation** results in a **narrower** curve, meaning data is concentrated around the mean.
     - A **large standard deviation** results in a **wider** curve, meaning data is more spread out.

6. **The Area Under the Curve:**
   The total area under the normal distribution curve is equal to 1. This represents the fact that the total probability of all possible outcomes in a random experiment is 1.

7. **Standard Normal Distribution:**
   A special case of the normal distribution is the **Standard Normal Distribution**, where the mean \( \mu = 0 \) and the standard deviation \( \sigma = 1 \). The PDF for the standard normal distribution is:

   \[
   f(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}
   \]
   Where \( z \) represents the standardised variable (also known as the **z-score**).

   The z-score represents the number of standard deviations a data point is from the mean. For example, a z-score of 1 indicates that the value is 1 standard deviation above the mean.

---

#### **Applications of Normal Distribution**

1. **Statistical Inference:**
   The normal distribution is commonly used in hypothesis testing, confidence intervals, and other inferential statistics. Many statistical tests assume that the data follows a normal distribution, particularly for large sample sizes.

2. **Measurement and Process Control:**
   In quality control and process management, many processes are assumed to follow a normal distribution. For example, the measurement errors in instruments often follow a normal distribution.

3. **Natural Phenomena:**
   Many biological and physical measurements, such as human height, IQ scores, and the error in measurement, tend to follow a normal distribution.

4. **Financial Modeling:**
   The normal distribution is also used in financial models to represent returns on investments, assuming that the returns are symmetrically distributed.

---

#### 💡 **TIP:**  
To convert a normal distribution to the standard normal distribution, you can use the formula for the **z-score**:
\[
z = \frac{x - \mu}{\sigma}
\]
Where:
- \( x \) is the value of interest,
- \( \mu \) is the mean,
- \( \sigma \) is the standard deviation.

---

### Introduction to Markov Chains and Random Walks

#### **Markov Chains**

A **Markov chain** is a mathematical system that undergoes transitions from one state to another, in a sequence of steps, where the probability of each transition depends only on the current state. This is known as the **Markov property**: the future state is conditionally independent of the past states given the present state.

- **Formal Definition:**
  A Markov chain is a sequence of random variables \( X_1, X_2, X_3, \dots \), where the probability of transitioning to the next state depends only on the current state and not on the sequence of events that preceded it. Formally, for any sequence of states \( s_1, s_2, \dots, s_n \), the Markov property can be written as:
  \[
  P(X_{n+1} = s_{n+1} | X_n = s_n, X_{n-1} = s_{n-1}, \dots, X_1 = s_1) = P(X_{n+1} = s_{n+1} | X_n = s_n)
  \]
  This means that the future depends only on the present, not on the past.

#### **Key Components of a Markov Chain**

1. **States**: The possible positions or conditions in which the system can be. States can be discrete or continuous, but in most basic examples, they are discrete.
   
2. **Transition Matrix**: A matrix that describes the probabilities of transitioning from one state to another. For a discrete set of states, a **transition matrix** \( P \) is typically used, where each entry \( P_{ij} \) represents the probability of moving from state \( i \) to state \( j \).

3. **Initial Distribution**: The probability distribution over the starting states. This tells us the likelihood of starting in any of the states of the system.

4. **Stationary Distribution**: A distribution over the states that remains unchanged after applying the transition matrix repeatedly. If a Markov chain reaches a stationary distribution, the probability of being in a particular state remains constant over time.

#### **Applications of Markov Chains**

- **Weather Modeling**: Predicting the weather conditions (sunny, rainy, etc.) over time, where the weather today only depends on the weather yesterday.
- **Queueing Systems**: Modeling the state of customers in a queue (e.g., how many people are waiting for service).
- **PageRank Algorithm**: Google’s algorithm for ranking web pages is based on Markov chains, where the states represent web pages and the transitions represent links between them.

---

#### **Random Walks**

A **random walk** is a special case of a Markov chain where the transitions depend on random choices. In a random walk, at each step, the process makes a decision based on some random event, like flipping a coin or rolling a die, to decide the next state. It is called a "walk" because the process typically moves in discrete steps through a state space, often visualized as a path or walk.

- **Simple Random Walk**:  
  A simple random walk can be defined on the number line, where the walker moves one step to the left or right at each time step with equal probability (i.e., 50% left and 50% right). Mathematically, if \( X_n \) is the position of the walker at time \( n \), then:
  \[
  X_{n+1} = X_n + \epsilon_n
  \]
  Where \( \epsilon_n \) is a random variable that takes values \( \pm 1 \) with equal probability.

- **Key Properties of Random Walks**:
  1. **Markov Property**: A random walk has the Markov property, as the future state depends only on the current state.
  2. **Symmetry**: In a simple random walk, the probability of moving left is the same as the probability of moving right.
  3. **Expected Position**: In a symmetric random walk, the expected position of the walker at any time is always the starting position (i.e., \( E[X_n] = 0 \) if starting at \( X_0 = 0 \)).

- **Types of Random Walks**:
  - **1D Random Walk**: The walker moves in a one-dimensional space, such as on a number line.
  - **2D or 3D Random Walk**: The walker moves in a two-dimensional plane or three-dimensional space.
  - **Non-Symmetric Random Walk**: In some cases, the walker may have different probabilities for moving in different directions.

#### **Applications of Random Walks**

1. **Stock Market Models**: Stock prices can be modeled using random walks, where the changes in stock prices are random and unpredictable over time.
   
2. **Physical Systems**: Random walks are used to model particle diffusion, such as the movement of molecules in a liquid or gas.

3. **Social Networks**: Random walks can be used to simulate how information or influence spreads through a network.

4. **Pathfinding Algorithms**: Random walks are sometimes used in algorithms to explore graphs, such as in random search methods for optimization.

---

### Descriptive, Predictive, and Prescriptive Statistics

#### **Descriptive Statistics**

**Descriptive statistics** involves the use of numerical and graphical methods to describe and summarize the features of a dataset. It provides a clear overview of the data without making predictions or inferences.

- **Measures of Central Tendency**:
  These are statistical measures that describe the centre of a dataset. The most common measures include:
  - **Mean**: The average of all data points.
  - **Median**: The middle value when the data points are arranged in ascending order.
  - **Mode**: The most frequently occurring value in the dataset.

- **Measures of Dispersion**:
  These statistics describe the spread of data points. Common measures include:
  - **Range**: The difference between the maximum and minimum values.
  - **Variance**: The average squared deviation from the mean.
  - **Standard Deviation**: The square root of the variance, representing the spread of data in the same units as the original data.
  
- **Data Visualisation**:
  Graphical methods are also crucial in descriptive statistics for understanding data. Common visualizations include:
  - **Histograms**: Show the frequency distribution of data.
  - **Boxplots**: Display the data's distribution and highlight outliers.
  - **Bar charts** and **pie charts**: Useful for categorical data.

  **Example:**
  A company can use descriptive statistics to summarise sales data, calculating the average sales, the spread of sales figures, and visualizing sales trends over time.

#### **Predictive Statistics**

**Predictive statistics** uses historical data to make predictions about future events. It involves statistical models and machine learning techniques to forecast outcomes based on patterns observed in the data.

- **Regression Analysis**:
  - **Linear Regression**: Predicts a dependent variable based on the relationship between it and one or more independent variables. The relationship is represented by a linear equation.
  - **Multiple Regression**: Used when there are multiple independent variables.
  
- **Time Series Analysis**:
  Time series analysis involves data that is collected over time. It is used to predict future values based on past trends. Techniques like **ARIMA (AutoRegressive Integrated Moving Average)** are commonly used for time series forecasting.

- **Classification Models**:
  These models are used to predict categorical outcomes. Common methods include:
  - **Logistic Regression**: Predicts binary outcomes (e.g., yes/no, true/false).
  - **Decision Trees** and **Random Forests**: Use a tree-like model to predict outcomes based on input data.

- **Example**:  
  A retail business may use predictive analytics to forecast future sales based on historical sales data and seasonal trends.

#### **Prescriptive Statistics**

**Prescriptive statistics** goes a step further than predictive statistics by recommending actions based on data analysis. It suggests decisions that should be made in order to achieve specific objectives or optimise outcomes.

- **Optimization Models**:
  These models are used to find the best possible solution for a given problem. For example, **Linear Programming** is used in supply chain management to optimize costs while meeting constraints like supply and demand.

- **Simulation**:
  Simulation techniques are used to model complex systems and evaluate different strategies or decisions. **Monte Carlo simulations** are commonly used to assess risks and uncertainty in decision-making.

- **Decision Analysis**:
  Decision analysis involves evaluating different decision paths and outcomes. Techniques such as **decision trees** and **sensitivity analysis** are used to determine the best course of action.

- **Example**:
  A manufacturing company could use prescriptive analytics to optimize production schedules, determining the most efficient use of resources to meet customer demand while minimizing costs.

---

### Statistical Inference

#### **Introduction to Statistical Inference**

**Statistical inference** is the process of drawing conclusions about a population based on a sample of data. It allows us to make generalizations, predictions, and estimates using the information derived from a smaller subset of the population.

There are two main branches of statistical inference:

1. **Estimation**: Estimating unknown parameters of a population (e.g., mean, variance).
2. **Hypothesis Testing**: Testing claims or assumptions about a population parameter.

Statistical inference uses probability theory to make predictions and test hypotheses, relying heavily on the concept of sampling distributions and the Central Limit Theorem.

#### **Types of Statistical Inference**

1. **Point Estimation**:
   - A **point estimate** is a single value used to approximate a population parameter. 
   - Example: Using the sample mean as an estimate for the population mean.

2. **Interval Estimation (Confidence Intervals)**:
   - An **interval estimate** provides a range of values within which the population parameter is expected to lie, with a certain level of confidence. This is typically expressed as a **confidence interval**.
   - Example: A 95% confidence interval for the population mean, which means there's a 95% probability that the true population mean falls within this range.

   **Formula for Confidence Interval**:
   \[
   \mu \pm z_{\alpha/2} \left( \frac{\sigma}{\sqrt{n}} \right)
   \]
   where:
   - \( \mu \) is the sample mean,
   - \( z_{\alpha/2} \) is the z-value for the desired confidence level,
   - \( \sigma \) is the population standard deviation (or sample standard deviation if population is unknown),
   - \( n \) is the sample size.

#### **Hypothesis Testing**

**Hypothesis testing** involves testing a hypothesis about a population parameter using sample data. The goal is to assess whether the sample data provides sufficient evidence to reject the null hypothesis.

- **Null Hypothesis (H₀)**: The hypothesis that there is no effect or difference, and that any observed effect is due to sampling variability.
- **Alternative Hypothesis (H₁ or Ha)**: The hypothesis that there is an effect or difference.

##### **Steps in Hypothesis Testing**:

1. **State the Hypotheses**:  
   - Null hypothesis \( H_0 \) (e.g., \( H_0: \mu = 0 \)) and alternative hypothesis \( H_1 \) (e.g., \( H_1: \mu \neq 0 \)).

2. **Choose the Significance Level (α)**:  
   The significance level \( \alpha \) represents the probability of rejecting the null hypothesis when it is true (Type I error). Common values are 0.05 or 0.01.

3. **Calculate the Test Statistic**:  
   Depending on the type of test (e.g., t-test, z-test), the test statistic is calculated to compare the sample data to the hypothesized population parameter.

4. **Decision Rule**:  
   Using the test statistic and the significance level, decide whether to reject or fail to reject the null hypothesis. This is typically done by comparing the p-value to \( \alpha \):
   - If \( p \)-value < \( \alpha \), reject \( H_0 \).
   - If \( p \)-value > \( \alpha \), fail to reject \( H_0 \).

5. **Conclusion**:  
   Based on the decision rule, either reject the null hypothesis (supporting the alternative hypothesis) or fail to reject it.

##### **Example of Hypothesis Testing**:

Consider testing if a new drug has an effect on blood pressure:
- Null hypothesis \( H_0 \): The drug has no effect (mean difference = 0).
- Alternative hypothesis \( H_1 \): The drug has an effect (mean difference ≠ 0).
- After performing the test, if the p-value is 0.03 (less than 0.05), we reject the null hypothesis and conclude that the drug has an effect on blood pressure.

#### **Types of Errors in Hypothesis Testing**

- **Type I Error (False Positive)**: Rejecting the null hypothesis when it is actually true.
- **Type II Error (False Negative)**: Failing to reject the null hypothesis when it is actually false.

#### **Power of a Test**

The **power of a test** is the probability of correctly rejecting the null hypothesis when it is false. A higher power indicates a better ability to detect a true effect.

- **Power Formula**:  
  Power = \( 1 - \beta \), where \( \beta \) is the probability of a Type II error.
  
- Factors that affect the power of a test:
  - Sample size: Larger sample sizes increase the power.
  - Effect size: A larger effect size makes it easier to detect a difference.
  - Significance level \( \alpha \): A higher \( \alpha \) (e.g., 0.10) increases the power but also increases the risk of a Type I error.

---

### Populations and Samples

#### **Introduction**

In statistics, **populations** and **samples** are foundational concepts that help us collect and analyze data. A **population** is the entire group of individuals or items that we are interested in studying, while a **sample** is a subset of that population that is selected for analysis. Statistical methods often focus on drawing inferences about populations based on sample data.

#### **Population**

A **population** refers to the complete set of data or individuals that we want to study or analyze. This can include:

- **Individuals**: People, objects, or entities.
- **Measurements**: Characteristics like height, weight, or income.
- **Events**: All outcomes of a certain type (e.g., all test scores from a set of exams).

**Key Points**:
- A population includes all possible data points or units of interest.
- The parameters of a population are fixed but often unknown (e.g., population mean \( \mu \), population variance \( \sigma^2 \)).

**Example**:  
In a study to understand the average income of citizens in a city, the **population** is all the citizens of that city.

#### **Sample**

A **sample** is a smaller subset of the population that is selected for study. Since it is often impractical or impossible to gather data from an entire population, we collect a sample that is representative of the population. 

**Key Points**:
- A sample should be **randomly selected** to ensure it accurately represents the population, minimizing bias.
- A sample is typically used to estimate population parameters (e.g., using the sample mean \( \overline{x} \) to estimate the population mean \( \mu \)).
- The statistics calculated from a sample, such as the sample mean or sample variance, are estimates of the corresponding population parameters.

**Example**:  
If the study mentioned earlier about average income includes only 500 people from the city, then the **sample** consists of those 500 citizens.

#### **Differences Between Population and Sample**

| Aspect             | Population                               | Sample                            |
|--------------------|------------------------------------------|-----------------------------------|
| **Size**           | Entire group being studied               | Subset of the population          |
| **Parameter**      | Parameters are fixed but unknown (e.g., \( \mu \), \( \sigma \)) | Statistics are used to estimate parameters (e.g., sample mean) |
| **Cost**           | Often impractical to measure entire population | More cost-effective to measure a sample |
| **Data Analysis**  | Direct analysis of entire group          | Inference made based on sample data |

#### **Sampling Methods**

To ensure that the sample accurately represents the population, various **sampling techniques** can be used. These include:

1. **Random Sampling**: Every individual in the population has an equal chance of being selected.
   - **Example**: Drawing names randomly from a hat.
   
2. **Stratified Sampling**: The population is divided into subgroups (strata), and a random sample is taken from each stratum.
   - **Example**: In a city, dividing the population by age groups (e.g., 18-25, 26-40, 41-60, 60+) and sampling from each group.

3. **Systematic Sampling**: Every nth individual is selected from the population.
   - **Example**: Selecting every 10th person from a list of 1,000 names.

4. **Cluster Sampling**: The population is divided into clusters, and entire clusters are randomly selected for analysis.
   - **Example**: A survey of classrooms in a school, where entire classrooms (clusters) are selected randomly.

5. **Convenience Sampling**: The sample is selected based on ease of access rather than random selection, which may introduce bias.
   - **Example**: Surveying people in a nearby park or mall.

#### **Sampling Error**

A **sampling error** is the difference between the sample statistic and the population parameter. It occurs because the sample does not contain all the data points from the population. The error decreases as the sample size increases.

- **Formula for Sampling Error**:
  \[
  \text{Sampling Error} = \overline{x} - \mu
  \]
  where:
  - \( \overline{x} \) is the sample mean.
  - \( \mu \) is the population mean.

#### **Importance of Sampling**

- **Efficiency**: Sampling allows researchers to gather information from large populations in a more manageable and cost-effective way.
- **Inferences**: Through proper statistical techniques, samples allow us to make reliable inferences about the population as a whole.
  
#### **Bias in Sampling**

Bias refers to systematic errors that lead to inaccurate or unfair representations of the population. Common types of bias include:

- **Selection Bias**: Occurs when the sample is not representative of the population due to improper selection methods.
- **Nonresponse Bias**: Happens when a significant portion of the selected sample does not respond, skewing the results.
- **Measurement Bias**: Arises when the data collection process distorts the information being collected.

---

### Statistical Modelling

#### **Introduction to Statistical Modelling**

**Statistical modelling** is the process of creating mathematical models that represent real-world phenomena using statistical techniques. A statistical model uses data to represent relationships between variables, enabling predictions, inferences, and decision-making. Models are used to explain or predict the behavior of a system based on observed data.

**Key Elements of Statistical Modelling**:
- **Variables**: These are the quantities that can take different values. There are two types:
  - **Independent Variables**: Variables that explain or predict changes in the dependent variable (e.g., age, income).
  - **Dependent Variable**: The variable being predicted or explained (e.g., test score, sales).
- **Parameters**: These are the constants that describe the relationship between variables in the model.
- **Data**: The input used to build the model, consisting of observed values for the variables.

#### **Types of Statistical Models**

1. **Linear Models**:
   - A **linear model** describes a linear relationship between the dependent variable and one or more independent variables.
   - The simplest form is a **linear regression model** where the dependent variable is predicted as a linear combination of the independent variables.
   
   **Example**:  
   Predicting a person’s salary (\( Y \)) based on their years of experience (\( X \)):
   \[
   Y = \beta_0 + \beta_1X + \epsilon
   \]
   Where:
   - \( Y \) = Salary,
   - \( X \) = Years of experience,
   - \( \beta_0 \) = Intercept,
   - \( \beta_1 \) = Slope (change in salary per year of experience),
   - \( \epsilon \) = Error term (random noise).

2. **Non-Linear Models**:
   - **Non-linear models** capture more complex relationships where the dependent variable does not change linearly with the independent variables.
   - These models are used when data shows a curvilinear or more complex pattern.
   
   **Example**:  
   Logistic regression, where the relationship between the independent variable and the dependent variable is modeled as a logistic function (used for binary outcomes).

3. **Time Series Models**:
   - **Time series models** are used to model data points collected at successive time intervals.
   - They are widely used in economics, finance, and weather forecasting.
   
   **Example**:  
   Predicting stock prices or sales over time, using models like ARIMA (AutoRegressive Integrated Moving Average).

4. **Multivariate Models**:
   - **Multivariate models** are used when more than one dependent variable is being predicted simultaneously, or when there are multiple independent variables.
   
   **Example**:  
   Predicting both salary and job satisfaction based on years of experience, education, and job role.

5. **Bayesian Models**:
   - **Bayesian models** use Bayes' theorem to update the probability estimate for a hypothesis as more data becomes available.
   - These models incorporate prior knowledge and use it to adjust predictions as new evidence is gathered.

6. **Generalized Linear Models (GLM)**:
   - **GLMs** extend linear models by allowing the dependent variable to follow different probability distributions (e.g., binomial, Poisson).
   - Examples include logistic regression (for binary outcomes) and Poisson regression (for count data).

#### **Steps in Statistical Modelling**

1. **Data Collection and Exploration**:
   - Gather relevant data and explore it to understand the variables and relationships.
   - Visualisation techniques like scatter plots or histograms can help identify patterns.

2. **Model Selection**:
   - Choose an appropriate model based on the nature of the data (linear vs. non-linear, time series, etc.).
   - Select the independent and dependent variables.

3. **Model Fitting**:
   - Fit the chosen model to the data by estimating the model parameters.
   - Use techniques like least squares (for linear regression) or maximum likelihood estimation (for other models) to find the best-fitting parameters.

4. **Model Evaluation**:
   - Evaluate the model's performance using various statistical metrics:
     - **R-squared**: Measures how well the model fits the data (for linear models).
     - **Mean Squared Error (MSE)**: Measures the average of the squared differences between actual and predicted values.
     - **AIC/BIC**: Information criteria used to compare models and avoid overfitting.

5. **Model Refinement**:
   - Adjust the model as needed by adding more variables, changing the model type, or transforming the data to improve accuracy.

6. **Prediction and Inference**:
   - Once the model is validated, it can be used for prediction or inference.
   - Predictions involve using the model to forecast future values.
   - Inference involves making conclusions about the population based on the model, such as determining the effect of one variable on another.

#### **Advantages of Statistical Modelling**

- **Prediction**: Statistical models can be used to predict future outcomes or behavior based on historical data.
- **Understanding Relationships**: Models help in understanding the relationships between different variables, such as cause-and-effect relationships.
- **Decision Making**: In business, finance, healthcare, and other fields, models help in making data-driven decisions.

#### **Challenges in Statistical Modelling**

- **Overfitting**: When a model is too complex, it can fit the training data very well but fail to generalize to new data.
- **Multicollinearity**: When independent variables are highly correlated, it becomes difficult to estimate their individual effects accurately.
- **Missing Data**: Missing values in the dataset can affect the accuracy of the model and lead to biased results.
- **Assumptions**: Many statistical models rely on assumptions (e.g., normality, linearity). Violating these assumptions can lead to incorrect conclusions.
